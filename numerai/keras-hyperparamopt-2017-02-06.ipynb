{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#sess = tf.InteractiveSession()\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType ,DoubleType ,BooleanType, IntegerType\n",
    "from pyspark.sql.functions import udf ,col ,upper, avg\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.regression import GBTRegressor ,LinearRegression ,GeneralizedLinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors \n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.regression import GBTRegressor ,LinearRegression ,GeneralizedLinearRegression\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "from pyspark.ml.linalg import Vectors \n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc=SparkContext()\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For gpu accelerated NN training:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import Merge\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n",
      "\n",
      "0.692547113878\n",
      "0.69234696709\n",
      "0.69254754212\n",
      "\n",
      "0.690144655883\n",
      "0.689943758458\n",
      "0.690145520668\n",
      "\n",
      "0.204365433835\n",
      "9.99200722163e-16\n"
     ]
    }
   ],
   "source": [
    "print log_loss([0]*500  + [1]*500,[[.5,.5]]*1000)\n",
    "\n",
    "print \"\"\n",
    "print log_loss([0]*520  + [1]*480,[[.51,.49]]*1000)\n",
    "print log_loss([0]*520  + [1]*480,[[.52,.48]]*1000)\n",
    "print log_loss([0]*520  + [1]*480,[[.53,.47]]*1000)\n",
    "\n",
    "print \"\"\n",
    "print log_loss([0]*540  + [1]*460,[[.53,.47]]*1000)\n",
    "print log_loss([0]*540  + [1]*460,[[.54,.46]]*1000)\n",
    "print log_loss([0]*540  + [1]*460,[[.55,.45]]*1000)\n",
    "\n",
    "print \"\"\n",
    "print log_loss([0]*540  + [1]*460,[[.8,.2]]*540+([[.2,8]]*460))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that nvida drivers are working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 10 15:29:37 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 860M    Off  | 0000:01:00.0     Off |                  N/A |\r\n",
      "| N/A   47C    P0    N/A /  N/A |    627MiB /  4044MiB |      6%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1073    G   /usr/lib/xorg/Xorg                             337MiB |\r\n",
      "|    0      1881    G   compiz                                         164MiB |\r\n",
      "|    0      4719    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd   125MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set:\n",
      "108405\n",
      "length of tournament set:\n",
      "45822\n"
     ]
    }
   ],
   "source": [
    "#numerai_train_df     = sqlContext.read.option(\"header\",\"true\").csv('file:///home/garth/Downloads/numerai20170130/numerai_training_data.csv',inferSchema='true')\n",
    "#numerai_tournament_df= sqlContext.read.option(\"header\",\"true\").csv('file:///home/garth/Downloads/numerai20170130/numerai_tournament_data.csv',inferSchema='true')\n",
    "\n",
    "numerai_train_df     = sqlContext.read.option(\"header\",\"true\").csv('file:///home/garth/Downloads/numerai_data_2017-05-10/numerai_training_data.csv',inferSchema='true')\n",
    "numerai_tournament_df= sqlContext.read.option(\"header\",\"true\").csv('file:///home/garth/Downloads/numerai_data_2017-05-10/numerai_tournament_data.csv',inferSchema='true')\n",
    "                                                                   \n",
    "print \"length of training set:\"\n",
    "print (numerai_train_df.count())\n",
    "print \"length of tournament set:\"\n",
    "print (numerai_tournament_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------+\n",
      "|id    |era |data_type|feature1|feature2|feature3|feature4|feature5|feature6|feature7|feature8|feature9|feature10|feature11|feature12|feature13|feature14|feature15|feature16|feature17|feature18|feature19|feature20|feature21|target|\n",
      "+------+----+---------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------+\n",
      "|140721|era1|train    |0.26647 |0.42487 |0.81401 |0.22889 |0.27456 |0.55654 |0.5531  |0.71899 |0.20035 |0.62848  |0.18636  |0.26651  |0.18847  |0.77235  |0.55002  |0.20237  |0.79605  |0.82971  |0.45757  |0.69761  |0.53739  |1     |\n",
      "|6942  |era1|train    |0.41334 |0.47533 |0.71847 |0.40792 |0.32433 |0.55806 |0.59592 |0.5183  |0.25348 |0.49711  |0.31162  |0.37999  |0.32083  |0.72435  |0.63751  |0.29143  |0.6786   |0.70083  |0.59967  |0.53103  |0.47446  |1     |\n",
      "|97514 |era1|train    |0.48937 |0.5603  |0.5915  |0.46432 |0.42291 |0.54177 |0.53542 |0.50577 |0.3235  |0.58043  |0.4058   |0.46203  |0.42195  |0.62651  |0.51604  |0.42938  |0.56744  |0.60008  |0.46966  |0.50322  |0.42803  |1     |\n",
      "+------+----+---------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "____________\n",
      "+------+-----+----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------+\n",
      "|id    |era  |data_type |feature1|feature2|feature3|feature4|feature5|feature6|feature7|feature8|feature9|feature10|feature11|feature12|feature13|feature14|feature15|feature16|feature17|feature18|feature19|feature20|feature21|target|\n",
      "+------+-----+----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------+\n",
      "|79637 |era97|validation|0.54177 |0.64267 |0.54365 |0.53625 |0.43622 |0.4901  |0.59626 |0.4333  |0.27936 |0.59308  |0.46092  |0.54357  |0.46999  |0.65848  |0.61414  |0.4634   |0.51415  |0.54882  |0.54992  |0.39919  |0.33837  |1     |\n",
      "|77605 |era97|validation|0.46513 |0.55879 |0.61386 |0.48131 |0.3993  |0.41525 |0.54916 |0.50796 |0.30567 |0.60046  |0.40061  |0.49994  |0.39465  |0.66286  |0.58364  |0.38549  |0.59305  |0.61357  |0.52784  |0.46704  |0.37416  |1     |\n",
      "|109353|era97|validation|0.57906 |0.57836 |0.55062 |0.50582 |0.42531 |0.53203 |0.61312 |0.39767 |0.35144 |0.50209  |0.44545  |0.48268  |0.48369  |0.63943  |0.56807  |0.48388  |0.50785  |0.5563   |0.55099  |0.43983  |0.43469  |0     |\n",
      "+------+-----+----------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerai_train_df.show(3,False)\n",
    "print (\"____________________________________\")\n",
    "numerai_tournament_df.show(3,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert input for use in ml lib algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df=numerai_train_df.rdd.map(lambda x: (x.id,x.era,x.data_type,Vectors.dense([\n",
    "                x.feature1\n",
    "                ,x.feature2\n",
    "                ,x.feature3\n",
    "                ,x.feature4\n",
    "                ,x.feature5\n",
    "                ,x.feature6\n",
    "                ,x.feature7\n",
    "                ,x.feature8\n",
    "                ,x.feature9\n",
    "                ,x.feature10\n",
    "                ,x.feature11\n",
    "                ,x.feature12\n",
    "                ,x.feature13\n",
    "                ,x.feature14\n",
    "                ,x.feature15\n",
    "                ,x.feature16\n",
    "                ,x.feature17\n",
    "                ,x.feature18\n",
    "                ,x.feature19\n",
    "                ,x.feature20\n",
    "                ,x.feature21\n",
    "                ]),float(x.target))).toDF([\"id\",\"era\",\"type\",\"features\",\"label\"])\n",
    "\n",
    "tournament_df=numerai_tournament_df.rdd.map(lambda x: (x.id,x.era,x.data_type,Vectors.dense([\n",
    "                x.feature1\n",
    "                ,x.feature2\n",
    "                ,x.feature3\n",
    "                ,x.feature4\n",
    "                ,x.feature5\n",
    "                ,x.feature6\n",
    "                ,x.feature7\n",
    "                ,x.feature8\n",
    "                ,x.feature9\n",
    "                ,x.feature10\n",
    "                ,x.feature11\n",
    "                ,x.feature12\n",
    "                ,x.feature13\n",
    "                ,x.feature14\n",
    "                ,x.feature15\n",
    "                ,x.feature16\n",
    "                ,x.feature17\n",
    "                ,x.feature18\n",
    "                ,x.feature19\n",
    "                ,x.feature20\n",
    "                ,x.feature21\n",
    "                ]))).toDF([\"id\",\"era\",\"type\",\"features\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+--------------------+-----+\n",
      "|    id| era| type|            features|label|\n",
      "+------+----+-----+--------------------+-----+\n",
      "|140721|era1|train|[0.26647,0.42487,...|  1.0|\n",
      "|  6942|era1|train|[0.41334,0.47533,...|  1.0|\n",
      "| 97514|era1|train|[0.48937,0.5603,0...|  1.0|\n",
      "| 60256|era1|train|[0.61195,0.65958,...|  1.0|\n",
      "| 30143|era1|train|[0.43758,0.50085,...|  1.0|\n",
      "| 91955|era1|train|[0.46868,0.52025,...|  0.0|\n",
      "| 73822|era1|train|[0.47625,0.74148,...|  0.0|\n",
      "|141352|era1|train|[0.49953,0.5555,0...|  0.0|\n",
      "|  2202|era1|train|[0.35671,0.37318,...|  0.0|\n",
      "|  1860|era1|train|[0.52146,0.51701,...|  1.0|\n",
      "|117896|era1|train|[0.43264,0.53669,...|  1.0|\n",
      "|128292|era1|train|[0.52781,0.48414,...|  1.0|\n",
      "| 91101|era1|train|[0.80581,0.59023,...|  1.0|\n",
      "|110379|era1|train|[0.55122,0.60682,...|  1.0|\n",
      "|125670|era1|train|[0.60113,0.27452,...|  1.0|\n",
      "| 24197|era1|train|[0.50609,0.50197,...|  0.0|\n",
      "| 90893|era1|train|[0.39845,0.46492,...|  1.0|\n",
      "| 47735|era1|train|[0.48893,0.54007,...|  0.0|\n",
      "|109070|era1|train|[0.49799,0.61928,...|  1.0|\n",
      "| 57380|era1|train|[0.56041,0.53916,...|  0.0|\n",
      "+------+----+-----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----+----------+--------------------+\n",
      "|    id|  era|      type|            features|\n",
      "+------+-----+----------+--------------------+\n",
      "| 79637|era97|validation|[0.54177,0.64267,...|\n",
      "| 77605|era97|validation|[0.46513,0.55879,...|\n",
      "|109353|era97|validation|[0.57906,0.57836,...|\n",
      "| 52332|era97|validation|[0.49406,0.57476,...|\n",
      "| 86969|era97|validation|[0.59719,0.45569,...|\n",
      "|109365|era97|validation|[0.56364,0.64025,...|\n",
      "| 35495|era97|validation|[0.45362,0.4399,0...|\n",
      "| 82022|era97|validation|[0.52506,0.63593,...|\n",
      "| 18111|era97|validation|[0.52006,0.49208,...|\n",
      "|   483|era97|validation|[0.54934,0.34994,...|\n",
      "|150968|era97|validation|[0.72093,0.71866,...|\n",
      "|146811|era97|validation|[0.36831,0.47837,...|\n",
      "| 63072|era97|validation|[0.52148,0.65438,...|\n",
      "| 11818|era97|validation|[0.43734,0.64828,...|\n",
      "|110484|era97|validation|[0.5614,0.71919,0...|\n",
      "|    63|era97|validation|[0.48226,0.55015,...|\n",
      "| 57784|era97|validation|[0.48568,0.53975,...|\n",
      "| 57135|era97|validation|[0.50057,0.5345,0...|\n",
      "| 99871|era97|validation|[0.53421,0.4781,0...|\n",
      "|151799|era97|validation|[0.52289,0.56166,...|\n",
      "+------+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()\n",
    "tournament_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(trainingData, testData) = train_df.randomSplit([0.6, 0.4],123456)\n",
    "(trainingData, testData,everythingElse) = train_df.randomSplit([0.2, 0.1, 0.7],12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define log loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def logloss(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do PCA and rename REPLACE the features column\n",
    "\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(trainingData)\n",
    "trainingDataPCA = model.transform(trainingData).drop(\"features\")\n",
    "trainingDataPCA = trainingDataPCA.withColumn(\"features\",trainingDataPCA.pcaFeatures).drop(\"pcaFeatures\")\n",
    "\n",
    "testingDataPCA = model.transform(testData).drop(\"features\")\n",
    "testingDataPCA = testingDataPCA.withColumn(\"features\",testingDataPCA.pcaFeatures).drop(\"pcaFeatures\")\n",
    "\n",
    "tournamentDataPCA = model.transform(tournament_df).drop(\"features\")\n",
    "tournamentDataPCA = tournamentDataPCA.withColumn(\"features\",tournamentDataPCA.pcaFeatures).drop(\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testingDataPCA.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData = trainingDataPCA\n",
    "testData = testingDataPCA\n",
    "tournament_df =tournamentDataPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=10\n",
    "#k=5\n",
    "#k=50\n",
    "threshold1=0.69\n",
    "threshold2=0.692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(k).setSeed(1)\n",
    "model = kmeans.fit(trainingData)\n",
    "wssse = model.computeCost(trainingData)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise for the number of clusters by finding the inflection point in cost vs number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clustered_train_df = model.transform(trainingData)\n",
    "clustered_train_df = clustered_train_df.withColumn(\"clusterNum\",clustered_train_df.prediction).drop(\"prediction\")\n",
    "clustered_train_df.show(1)\n",
    "\n",
    "clustered_test_df = model.transform(testData)\n",
    "clustered_test_df = clustered_test_df.withColumn(\"clusterNum\",clustered_test_df.prediction).drop(\"prediction\")\n",
    "clustered_test_df.show(1)\n",
    "\n",
    "clustered_tournament_df = model.transform(tournament_df)\n",
    "clustered_tournament_df = clustered_tournament_df.withColumn(\"clusterNum\",clustered_tournament_df.prediction).drop(\"prediction\")\n",
    "clustered_tournament_df.show(1)\n",
    "\n",
    "\n",
    "clustered_test_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Net with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"features\", \"clusterNum\"],\n",
    "    outputCol=\"features2\")\n",
    "clustered_train_df2 = assembler.transform(clustered_train_df)\n",
    "clustered_test_df2  = assembler.transform(clustered_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clustered_train_df2.show(1)\n",
    "clustered_test_df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features    = np.array(clustered_train_df2.select(\"features2\").toPandas())\n",
    "labels      = np.array(clustered_train_df2.select(\"label\").toPandas())\n",
    "\n",
    "featuresT    = np.array(clustered_test_df2.select(\"features2\").toPandas())\n",
    "labelsT      = np.array(clustered_test_df2.select(\"label\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_np     = np.array(map(lambda x: np.array(map(lambda y: np.array(y),x)), features))[:,][:][:]\n",
    "#train_np_pca = np.array(map(lambda x: np.array(map(lambda y: np.array(y),x)), pcaFeatures))[:,][:][:]\n",
    "train_res= np.array(map(lambda x: np.array(map(lambda y: np.array(y),x)), labels))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_np_test     = np.array(map(lambda x: np.array(map(lambda y: np.array(y),x)), featuresT))[:,0][:][:]\n",
    "#train_np_pca_test = np.array(map(lambda x: np.array(map(lambda y: np.array(y),x)), pcaFeaturesT))[:,0][:][:]\n",
    "train_res_test    = np.array(map(lambda x: np.array(map(lambda y: np.array(y),x)), labelsT))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(train_np[0])\n",
    "print train_np[10]\n",
    "\n",
    "print train_res[10]\n",
    "sum1= float(sum(train_res))\n",
    "len1= float(len(train_res))\n",
    "print \"ratio: \"+str(sum1/len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_np.shape\n",
    "train_np=train_np.reshape(train_np.shape[0],train_np.shape[2])\n",
    "print train_np.shape\n",
    "\n",
    "#print train_np_pca.shape\n",
    "#train_np_pca=train_np_pca.reshape(train_np_pca.shape[0],train_np_pca.shape[2])\n",
    "#print train_np_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(11, input_dim=11, init='uniform', activation='relu'))\n",
    "    model.add(Dense(11, input_dim=8, init='uniform', activation='relu'))\n",
    "    model.add(Dense(11, init='uniform', activation='relu'))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"/home/garth/Downloads/pima-indians-diabetes.data.txt\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.fit(train_np, train_res, nb_epoch=60, batch_size=1200)\n",
    "\n",
    "evaluated_train = model.evaluate(train_np,train_res)\n",
    "evaluated_test  = model.evaluate(train_np_test,train_res_test)\n",
    "\n",
    "nn_answer       = model.predict(train_np_test)\n",
    "\n",
    "print \"\"\n",
    "print \"---------------Train-------------------\"\n",
    "print \"logloss : \"+str(evaluated_train[0])\n",
    "print \"accuracy: \"+str(evaluated_train[1]*100)+\"%\"\n",
    "print \"---------------Test--------------------\"\n",
    "print \"logloss : \"+str(evaluated_test[0])\n",
    "print \"accuracy: \"+str(evaluated_test[1]*100)+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print nn_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier()\n",
    "\n",
    "#model.fit(train_np_pca, train_res)\n",
    "model.fit(train_np, train_res)\n",
    "\n",
    "# make predictions for test data\n",
    "#y_pred = model.predict(train_np_pca_test)\n",
    "y_pred = model.predict(train_np_test)\n",
    "\n",
    "xgb_predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(train_res_test, xgb_predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(np.array(xgb_predictions))\n",
    "print type(nn_answer)\n",
    "\n",
    "nn_answer=nn_answer.reshape(nn_answer.shape[0],)\n",
    "\n",
    "print np.array(xgb_predictions)\n",
    "print nn_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData.show()\n",
    "\n",
    "train_xgb_udf = udf(lambda a: model.predict(a) , DoubleType())\n",
    "\n",
    "trainingData2=trainingData.withColumn(\"xgboost_pred\",train_xgb_udf(trainingData.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_keras_udf = udf(lambda a,b: final_model. , DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData3=trainingData.withColumn(\"keras_pred\",train_keras_udf(trainingData.features,trainingData.pcaFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluated  = final_model.predict([train_np_test,train_np_pca_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluated.shape\n",
    "print train_res_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluated=evaluated.reshape(evaluated.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainingData2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "import copy\n",
    "\n",
    "def custom_get_params(self, **params):\n",
    "    res = copy.deepcopy(self.sk_params)\n",
    "    res.update({'build_fn': self.build_fn})\n",
    "    return res\n",
    "\n",
    "BaseWrapper.get_params = custom_get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "JoblibInternalError",
     "evalue": "JoblibInternalError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    169     pkg_name = mod_name.rpartition('.')[0]\n    170     main_globals = sys.modules[\"__main__\"].__dict__\n    171     if alter_argv:\n    172         sys.argv[0] = fname\n    173     return _run_code(code, main_globals, None,\n--> 174                      \"__main__\", fname, loader, pkg_name)\n        fname = '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    175 \n    176 def run_module(mod_name, init_globals=None,\n    177                run_name=None, alter_sys=False):\n    178     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    648 \n    649         If a global instance already exists, this reinitializes and starts it\n    650         \"\"\"\n    651         app = cls.instance(**kwargs)\n    652         app.initialize(argv)\n--> 653         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    654 \n    655 #-----------------------------------------------------------------------------\n    656 # utility functions, for convenience\n    657 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:15:17.193190', u'msg_id': u'7138DF19BB5945648D29B48D66F82689', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'7138DF19BB5945648D29B48D66F82689', 'msg_type': u'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['B64E8C726E0848628D7C1B41F48E4D5D']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:15:17.193190', u'msg_id': u'7138DF19BB5945648D29B48D66F82689', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'7138DF19BB5945648D29B48D66F82689', 'msg_type': u'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['B64E8C726E0848628D7C1B41F48E4D5D'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:15:17.193190', u'msg_id': u'7138DF19BB5945648D29B48D66F82689', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'7138DF19BB5945648D29B48D66F82689', 'msg_type': u'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.FunctionDef object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Print object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.For object>], cell_name='<ipython-input-3-d3391b681160>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<ExecutionResult object at 7f3e984bc110, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f3e3e69ee30, file \"<ipython-input-3-d3391b681160>\", line 25>\n        result = <ExecutionResult object at 7f3e984bc110, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f3e3e69ee30, file \"<ipython-input-3-d3391b681160>\", line 25>, result=<ExecutionResult object at 7f3e984bc110, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f3e3e69ee30, file \"<ipython-input-3-d3391b681160>\", line 25>\n        self.user_global_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n        self.user_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/garth/numerai_notebooks/<ipython-input-3-d3391b681160> in <module>()\n     20 # define the grid search parameters\n     21 batch_size = [10, 20, 40, 60, 80, 100]\n     22 epochs = [10, 50, 100]\n     23 param_grid = dict(batch_size=batch_size, nb_epoch=epochs)\n     24 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n---> 25 grid_result = grid.fit(X, Y)\n     26 # summarize results\n     27 print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n     28 means = grid_result.cv_results_['mean_test_score']\n     29 stds = grid_result.cv_results_['std_test_score']\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=0)>\n        X = array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]])\n        y = array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.])\n        groups = None\n        self.param_grid = {'batch_size': [10, 20, 40, 60, 80, 100], 'nb_epoch': [10, 50, 100]}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nInternalError                                      Mon Feb  6 15:15:18 2017\nPID: 7846                                    Python 2.7.12: /usr/bin/python\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'batch_size': 10, 'nb_epoch': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n        self.items = [(<function _fit_and_score>, (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'batch_size': 10, 'nb_epoch': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=<keras.wrappers.scikit_learn.KerasClassifier object>, X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), scorer=<function _passthrough_scorer>, train=array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), verbose=0, parameters={'batch_size': 10, 'nb_epoch': 10}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method KerasClassifier.fit of <keras.wrappers.scikit_learn.KerasClassifier object>>\n        X_train = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y_train = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py in fit(self=<keras.wrappers.scikit_learn.KerasClassifier object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), **kwargs={})\n    140             y = to_categorical(y)\n    141 \n    142         fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n    143         fit_args.update(kwargs)\n    144 \n--> 145         history = self.model.fit(x, y, **fit_args)\n        history = undefined\n        self.model.fit = <bound method Sequential.fit of <keras.models.Sequential object>>\n        x = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_args = {'batch_size': 10, 'nb_epoch': 10, 'verbose': 0}\n    146 \n    147         return history\n    148 \n    149     def filter_sk_params(self, fn, override=None):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/models.py in fit(self=<keras.models.Sequential object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), batch_size=10, nb_epoch=10, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, **kwargs={})\n    667                               validation_split=validation_split,\n    668                               validation_data=validation_data,\n    669                               shuffle=shuffle,\n    670                               class_weight=class_weight,\n    671                               sample_weight=sample_weight,\n--> 672                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n    673 \n    674     def evaluate(self, x, y, batch_size=32, verbose=1,\n    675                  sample_weight=None, **kwargs):\n    676         \"\"\"Computes the loss on some input data, batch by batch.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in fit(self=<keras.engine.training.Model object>, x=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])], y=[array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]])], batch_size=10, nb_epoch=10, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n   1187         return self._fit_loop(f, ins, out_labels=out_labels,\n   1188                               batch_size=batch_size, nb_epoch=nb_epoch,\n   1189                               verbose=verbose, callbacks=callbacks,\n   1190                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n   1191                               callback_metrics=callback_metrics,\n-> 1192                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n   1193 \n   1194     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n   1195         \"\"\"Returns the loss value and metrics values for the model\n   1196         in test mode. Computation is done in batches.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in _fit_loop(self=<keras.engine.training.Model object>, f=<keras.backend.tensorflow_backend.Function object>, ins=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  ...,\n        1.,  1.,  1.,  1.,  1.], dtype=float32)], out_labels=['loss', 'acc'], batch_size=10, nb_epoch=10, verbose=0, callbacks=<keras.callbacks.CallbackList object>, val_f=None, val_ins=None, shuffle=True, callback_metrics=['loss', 'acc'], initial_epoch=0)\n    887                                     'pass shuffle=\"batch\".')\n    888                 batch_logs = {}\n    889                 batch_logs['batch'] = batch_index\n    890                 batch_logs['size'] = len(batch_ids)\n    891                 callbacks.on_batch_begin(batch_index, batch_logs)\n--> 892                 outs = f(ins_batch)\n        outs = undefined\n        f = <keras.backend.tensorflow_backend.Function object>\n        ins_batch = [array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)]\n    893                 if not isinstance(outs, list):\n    894                     outs = [outs]\n    895                 for l, o in zip(out_labels, outs):\n    896                     batch_logs[l] = o\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py in __call__(self=<keras.backend.tensorflow_backend.Function object>, inputs=[array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)])\n   1893                 sparse_coo = value.tocoo()\n   1894                 indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n   1895                                           np.expand_dims(sparse_coo.col, 1)), 1)\n   1896                 value = (indices, sparse_coo.data, sparse_coo.shape)\n   1897             feed_dict[tensor] = value\n-> 1898         session = get_session()\n        session = undefined\n   1899         updated = session.run(self.outputs + [self.updates_op],\n   1900                               feed_dict=feed_dict)\n   1901         return updated[:len(self.outputs)]\n   1902 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py in get_session()\n    110                 config = tf.ConfigProto(allow_soft_placement=True)\n    111             else:\n    112                 nb_thread = int(os.environ.get('OMP_NUM_THREADS'))\n    113                 config = tf.ConfigProto(intra_op_parallelism_threads=nb_thread,\n    114                                         allow_soft_placement=True)\n--> 115             _SESSION = tf.Session(config=config)\n        config = allow_soft_placement: true\n\n    116         session = _SESSION\n    117     if not _MANUAL_VAR_INIT:\n    118         _initialize_variables()\n    119     return session\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in __init__(self=<tensorflow.python.client.session.Session object>, target='', graph=None, config=allow_soft_placement: true\n)\n   1181       graph: (Optional.) The `Graph` to be launched (described above).\n   1182       config: (Optional.) A [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto)\n   1183         protocol buffer with configuration options for the session.\n   1184 \n   1185     \"\"\"\n-> 1186     super(Session, self).__init__(target, graph, config=config)\n        self.__init__ = <bound method Session.__init__ of <tensorflow.python.client.session.Session object>>\n        target = ''\n        graph = None\n        config = allow_soft_placement: true\n\n   1187     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\n   1188     self._default_graph_context_manager = None\n   1189     self._default_session_context_manager = None\n   1190 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in __init__(self=<tensorflow.python.client.session.Session object>, target='', graph=None, config=allow_soft_placement: true\n)\n    546 \n    547     self._session = None\n    548     opts = tf_session.TF_NewSessionOptions(target=self._target, config=config)\n    549     try:\n    550       with errors.raise_exception_on_not_ok_status() as status:\n--> 551         self._session = tf_session.TF_NewDeprecatedSession(opts, status)\n        self._session = None\n        opts = <Swig Object of type 'TF_SessionOptions *'>\n        status = <Swig Object of type 'TF_Status *'>\n    552     finally:\n    553       tf_session.TF_DeleteSessionOptions(opts)\n    554 \n    555   def close(self):\n\n...........................................................................\n/usr/lib/python2.7/contextlib.py in __exit__(self=<contextlib.GeneratorContextManager object>, type=None, value=None, traceback=None)\n     19             raise RuntimeError(\"generator didn't yield\")\n     20 \n     21     def __exit__(self, type, value, traceback):\n     22         if type is None:\n     23             try:\n---> 24                 self.gen.next()\n        self.gen.next = <method-wrapper 'next' of generator object>\n     25             except StopIteration:\n     26                 return\n     27             else:\n     28                 raise RuntimeError(\"generator didn't stop\")\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\n    464     yield status\n    465     if pywrap_tensorflow.TF_GetCode(status) != 0:\n    466       raise _make_specific_exception(\n    467           None, None,\n    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),\n--> 469           pywrap_tensorflow.TF_GetCode(status))\n        status = <Swig Object of type 'TF_Status *'>\n    470   finally:\n    471     pywrap_tensorflow.TF_DeleteStatus(status)\n    472 \n    473 \n\nInternalError: Failed to create session.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJoblibInternalError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d3391b681160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibInternalError\u001b[0m: JoblibInternalError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    169     pkg_name = mod_name.rpartition('.')[0]\n    170     main_globals = sys.modules[\"__main__\"].__dict__\n    171     if alter_argv:\n    172         sys.argv[0] = fname\n    173     return _run_code(code, main_globals, None,\n--> 174                      \"__main__\", fname, loader, pkg_name)\n        fname = '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    175 \n    176 def run_module(mod_name, init_globals=None,\n    177                run_name=None, alter_sys=False):\n    178     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    648 \n    649         If a global instance already exists, this reinitializes and starts it\n    650         \"\"\"\n    651         app = cls.instance(**kwargs)\n    652         app.initialize(argv)\n--> 653         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    654 \n    655 #-----------------------------------------------------------------------------\n    656 # utility functions, for convenience\n    657 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:15:17.193190', u'msg_id': u'7138DF19BB5945648D29B48D66F82689', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'7138DF19BB5945648D29B48D66F82689', 'msg_type': u'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['B64E8C726E0848628D7C1B41F48E4D5D']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:15:17.193190', u'msg_id': u'7138DF19BB5945648D29B48D66F82689', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'7138DF19BB5945648D29B48D66F82689', 'msg_type': u'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['B64E8C726E0848628D7C1B41F48E4D5D'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:15:17.193190', u'msg_id': u'7138DF19BB5945648D29B48D66F82689', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'7138DF19BB5945648D29B48D66F82689', 'msg_type': u'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.FunctionDef object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Print object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.For object>], cell_name='<ipython-input-3-d3391b681160>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<ExecutionResult object at 7f3e984bc110, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f3e3e69ee30, file \"<ipython-input-3-d3391b681160>\", line 25>\n        result = <ExecutionResult object at 7f3e984bc110, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f3e3e69ee30, file \"<ipython-input-3-d3391b681160>\", line 25>, result=<ExecutionResult object at 7f3e984bc110, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f3e3e69ee30, file \"<ipython-input-3-d3391b681160>\", line 25>\n        self.user_global_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n        self.user_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/garth/numerai_notebooks/<ipython-input-3-d3391b681160> in <module>()\n     20 # define the grid search parameters\n     21 batch_size = [10, 20, 40, 60, 80, 100]\n     22 epochs = [10, 50, 100]\n     23 param_grid = dict(batch_size=batch_size, nb_epoch=epochs)\n     24 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n---> 25 grid_result = grid.fit(X, Y)\n     26 # summarize results\n     27 print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n     28 means = grid_result.cv_results_['mean_test_score']\n     29 stds = grid_result.cv_results_['std_test_score']\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=0)>\n        X = array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]])\n        y = array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.])\n        groups = None\n        self.param_grid = {'batch_size': [10, 20, 40, 60, 80, 100], 'nb_epoch': [10, 50, 100]}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nInternalError                                      Mon Feb  6 15:15:18 2017\nPID: 7846                                    Python 2.7.12: /usr/bin/python\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'batch_size': 10, 'nb_epoch': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n        self.items = [(<function _fit_and_score>, (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'batch_size': 10, 'nb_epoch': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=<keras.wrappers.scikit_learn.KerasClassifier object>, X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), scorer=<function _passthrough_scorer>, train=array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), verbose=0, parameters={'batch_size': 10, 'nb_epoch': 10}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method KerasClassifier.fit of <keras.wrappers.scikit_learn.KerasClassifier object>>\n        X_train = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y_train = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py in fit(self=<keras.wrappers.scikit_learn.KerasClassifier object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), **kwargs={})\n    140             y = to_categorical(y)\n    141 \n    142         fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n    143         fit_args.update(kwargs)\n    144 \n--> 145         history = self.model.fit(x, y, **fit_args)\n        history = undefined\n        self.model.fit = <bound method Sequential.fit of <keras.models.Sequential object>>\n        x = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_args = {'batch_size': 10, 'nb_epoch': 10, 'verbose': 0}\n    146 \n    147         return history\n    148 \n    149     def filter_sk_params(self, fn, override=None):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/models.py in fit(self=<keras.models.Sequential object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), batch_size=10, nb_epoch=10, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, **kwargs={})\n    667                               validation_split=validation_split,\n    668                               validation_data=validation_data,\n    669                               shuffle=shuffle,\n    670                               class_weight=class_weight,\n    671                               sample_weight=sample_weight,\n--> 672                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n    673 \n    674     def evaluate(self, x, y, batch_size=32, verbose=1,\n    675                  sample_weight=None, **kwargs):\n    676         \"\"\"Computes the loss on some input data, batch by batch.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in fit(self=<keras.engine.training.Model object>, x=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])], y=[array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]])], batch_size=10, nb_epoch=10, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n   1187         return self._fit_loop(f, ins, out_labels=out_labels,\n   1188                               batch_size=batch_size, nb_epoch=nb_epoch,\n   1189                               verbose=verbose, callbacks=callbacks,\n   1190                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n   1191                               callback_metrics=callback_metrics,\n-> 1192                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n   1193 \n   1194     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n   1195         \"\"\"Returns the loss value and metrics values for the model\n   1196         in test mode. Computation is done in batches.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in _fit_loop(self=<keras.engine.training.Model object>, f=<keras.backend.tensorflow_backend.Function object>, ins=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  ...,\n        1.,  1.,  1.,  1.,  1.], dtype=float32)], out_labels=['loss', 'acc'], batch_size=10, nb_epoch=10, verbose=0, callbacks=<keras.callbacks.CallbackList object>, val_f=None, val_ins=None, shuffle=True, callback_metrics=['loss', 'acc'], initial_epoch=0)\n    887                                     'pass shuffle=\"batch\".')\n    888                 batch_logs = {}\n    889                 batch_logs['batch'] = batch_index\n    890                 batch_logs['size'] = len(batch_ids)\n    891                 callbacks.on_batch_begin(batch_index, batch_logs)\n--> 892                 outs = f(ins_batch)\n        outs = undefined\n        f = <keras.backend.tensorflow_backend.Function object>\n        ins_batch = [array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)]\n    893                 if not isinstance(outs, list):\n    894                     outs = [outs]\n    895                 for l, o in zip(out_labels, outs):\n    896                     batch_logs[l] = o\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py in __call__(self=<keras.backend.tensorflow_backend.Function object>, inputs=[array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)])\n   1893                 sparse_coo = value.tocoo()\n   1894                 indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n   1895                                           np.expand_dims(sparse_coo.col, 1)), 1)\n   1896                 value = (indices, sparse_coo.data, sparse_coo.shape)\n   1897             feed_dict[tensor] = value\n-> 1898         session = get_session()\n        session = undefined\n   1899         updated = session.run(self.outputs + [self.updates_op],\n   1900                               feed_dict=feed_dict)\n   1901         return updated[:len(self.outputs)]\n   1902 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py in get_session()\n    110                 config = tf.ConfigProto(allow_soft_placement=True)\n    111             else:\n    112                 nb_thread = int(os.environ.get('OMP_NUM_THREADS'))\n    113                 config = tf.ConfigProto(intra_op_parallelism_threads=nb_thread,\n    114                                         allow_soft_placement=True)\n--> 115             _SESSION = tf.Session(config=config)\n        config = allow_soft_placement: true\n\n    116         session = _SESSION\n    117     if not _MANUAL_VAR_INIT:\n    118         _initialize_variables()\n    119     return session\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in __init__(self=<tensorflow.python.client.session.Session object>, target='', graph=None, config=allow_soft_placement: true\n)\n   1181       graph: (Optional.) The `Graph` to be launched (described above).\n   1182       config: (Optional.) A [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto)\n   1183         protocol buffer with configuration options for the session.\n   1184 \n   1185     \"\"\"\n-> 1186     super(Session, self).__init__(target, graph, config=config)\n        self.__init__ = <bound method Session.__init__ of <tensorflow.python.client.session.Session object>>\n        target = ''\n        graph = None\n        config = allow_soft_placement: true\n\n   1187     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\n   1188     self._default_graph_context_manager = None\n   1189     self._default_session_context_manager = None\n   1190 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in __init__(self=<tensorflow.python.client.session.Session object>, target='', graph=None, config=allow_soft_placement: true\n)\n    546 \n    547     self._session = None\n    548     opts = tf_session.TF_NewSessionOptions(target=self._target, config=config)\n    549     try:\n    550       with errors.raise_exception_on_not_ok_status() as status:\n--> 551         self._session = tf_session.TF_NewDeprecatedSession(opts, status)\n        self._session = None\n        opts = <Swig Object of type 'TF_SessionOptions *'>\n        status = <Swig Object of type 'TF_Status *'>\n    552     finally:\n    553       tf_session.TF_DeleteSessionOptions(opts)\n    554 \n    555   def close(self):\n\n...........................................................................\n/usr/lib/python2.7/contextlib.py in __exit__(self=<contextlib.GeneratorContextManager object>, type=None, value=None, traceback=None)\n     19             raise RuntimeError(\"generator didn't yield\")\n     20 \n     21     def __exit__(self, type, value, traceback):\n     22         if type is None:\n     23             try:\n---> 24                 self.gen.next()\n        self.gen.next = <method-wrapper 'next' of generator object>\n     25             except StopIteration:\n     26                 return\n     27             else:\n     28                 raise RuntimeError(\"generator didn't stop\")\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\n    464     yield status\n    465     if pywrap_tensorflow.TF_GetCode(status) != 0:\n    466       raise _make_specific_exception(\n    467           None, None,\n    468           compat.as_text(pywrap_tensorflow.TF_Message(status)),\n--> 469           pywrap_tensorflow.TF_GetCode(status))\n        status = <Swig Object of type 'TF_Status *'>\n    470   finally:\n    471     pywrap_tensorflow.TF_DeleteStatus(status)\n    472 \n    473 \n\nInternalError: Failed to create session.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"/home/garth/Downloads/pima-indians-diabetes.data.txt\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, nb_epoch=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"/home/garth/Downloads/pima-indians-diabetes.data.txt\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, nb_epoch=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "JoblibInternalError",
     "evalue": "JoblibInternalError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    169     pkg_name = mod_name.rpartition('.')[0]\n    170     main_globals = sys.modules[\"__main__\"].__dict__\n    171     if alter_argv:\n    172         sys.argv[0] = fname\n    173     return _run_code(code, main_globals, None,\n--> 174                      \"__main__\", fname, loader, pkg_name)\n        fname = '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    175 \n    176 def run_module(mod_name, init_globals=None,\n    177                run_name=None, alter_sys=False):\n    178     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    648 \n    649         If a global instance already exists, this reinitializes and starts it\n    650         \"\"\"\n    651         app = cls.instance(**kwargs)\n    652         app.initialize(argv)\n--> 653         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    654 \n    655 #-----------------------------------------------------------------------------\n    656 # utility functions, for convenience\n    657 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:23:00.203236', u'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', 'msg_type': u'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['B64E8C726E0848628D7C1B41F48E4D5D']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:23:00.203236', u'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', 'msg_type': u'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['B64E8C726E0848628D7C1B41F48E4D5D'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:23:00.203236', u'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', 'msg_type': u'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Import object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.FunctionDef object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Print object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, ...], cell_name='<ipython-input-7-a401dd13464c>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<ExecutionResult object at 7f3e7ff2d310, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f3e7ff637b0, file \"<ipython-input-7-a401dd13464c>\", line 31>\n        result = <ExecutionResult object at 7f3e7ff2d310, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f3e7ff637b0, file \"<ipython-input-7-a401dd13464c>\", line 31>, result=<ExecutionResult object at 7f3e7ff2d310, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f3e7ff637b0, file \"<ipython-input-7-a401dd13464c>\", line 31>\n        self.user_global_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n        self.user_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/garth/numerai_notebooks/<ipython-input-7-a401dd13464c> in <module>()\n     26 model = KerasClassifier(build_fn=create_model, nb_epoch=100, batch_size=10, verbose=0)\n     27 # define the grid search parameters\n     28 optimizer = ['SGD', 'RMSprop', 'Adagrad']\n     29 param_grid = dict(optimizer=optimizer)\n     30 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n---> 31 grid_result = grid.fit(X, Y)\n     32 # summarize results\n     33 print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n     34 means = grid_result.cv_results_['mean_test_score']\n     35 stds = grid_result.cv_results_['std_test_score']\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=0)>\n        X = array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]])\n        y = array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.])\n        groups = None\n        self.param_grid = {'optimizer': ['SGD', 'RMSprop', 'Adagrad']}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nInternalError                                      Mon Feb  6 15:23:02 2017\nPID: 8503                                    Python 2.7.12: /usr/bin/python\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'optimizer': 'SGD'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n        self.items = [(<function _fit_and_score>, (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'optimizer': 'SGD'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=<keras.wrappers.scikit_learn.KerasClassifier object>, X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), scorer=<function _passthrough_scorer>, train=array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), verbose=0, parameters={'optimizer': 'SGD'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method KerasClassifier.fit of <keras.wrappers.scikit_learn.KerasClassifier object>>\n        X_train = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y_train = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py in fit(self=<keras.wrappers.scikit_learn.KerasClassifier object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), **kwargs={})\n    140             y = to_categorical(y)\n    141 \n    142         fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n    143         fit_args.update(kwargs)\n    144 \n--> 145         history = self.model.fit(x, y, **fit_args)\n        history = undefined\n        self.model.fit = <bound method Sequential.fit of <keras.models.Sequential object>>\n        x = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_args = {'batch_size': 10, 'nb_epoch': 100, 'verbose': 0}\n    146 \n    147         return history\n    148 \n    149     def filter_sk_params(self, fn, override=None):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/models.py in fit(self=<keras.models.Sequential object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), batch_size=10, nb_epoch=100, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, **kwargs={})\n    667                               validation_split=validation_split,\n    668                               validation_data=validation_data,\n    669                               shuffle=shuffle,\n    670                               class_weight=class_weight,\n    671                               sample_weight=sample_weight,\n--> 672                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n    673 \n    674     def evaluate(self, x, y, batch_size=32, verbose=1,\n    675                  sample_weight=None, **kwargs):\n    676         \"\"\"Computes the loss on some input data, batch by batch.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in fit(self=<keras.engine.training.Model object>, x=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])], y=[array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]])], batch_size=10, nb_epoch=100, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n   1187         return self._fit_loop(f, ins, out_labels=out_labels,\n   1188                               batch_size=batch_size, nb_epoch=nb_epoch,\n   1189                               verbose=verbose, callbacks=callbacks,\n   1190                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n   1191                               callback_metrics=callback_metrics,\n-> 1192                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n   1193 \n   1194     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n   1195         \"\"\"Returns the loss value and metrics values for the model\n   1196         in test mode. Computation is done in batches.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in _fit_loop(self=<keras.engine.training.Model object>, f=<keras.backend.tensorflow_backend.Function object>, ins=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  ...,\n        1.,  1.,  1.,  1.,  1.], dtype=float32)], out_labels=['loss', 'acc'], batch_size=10, nb_epoch=100, verbose=0, callbacks=<keras.callbacks.CallbackList object>, val_f=None, val_ins=None, shuffle=True, callback_metrics=['loss', 'acc'], initial_epoch=0)\n    887                                     'pass shuffle=\"batch\".')\n    888                 batch_logs = {}\n    889                 batch_logs['batch'] = batch_index\n    890                 batch_logs['size'] = len(batch_ids)\n    891                 callbacks.on_batch_begin(batch_index, batch_logs)\n--> 892                 outs = f(ins_batch)\n        outs = undefined\n        f = <keras.backend.tensorflow_backend.Function object>\n        ins_batch = [array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)]\n    893                 if not isinstance(outs, list):\n    894                     outs = [outs]\n    895                 for l, o in zip(out_labels, outs):\n    896                     batch_logs[l] = o\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py in __call__(self=<keras.backend.tensorflow_backend.Function object>, inputs=[array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)])\n   1895                                           np.expand_dims(sparse_coo.col, 1)), 1)\n   1896                 value = (indices, sparse_coo.data, sparse_coo.shape)\n   1897             feed_dict[tensor] = value\n   1898         session = get_session()\n   1899         updated = session.run(self.outputs + [self.updates_op],\n-> 1900                               feed_dict=feed_dict)\n        feed_dict = {<tf.Tensor 'dense_input_1:0' shape=(?, 8) dtype=float32>: array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), <tf.Tensor 'dense_2_target:0' shape=(?, ?) dtype=float32>: array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), <tf.Tensor 'dense_2_sample_weights:0' shape=(?,) dtype=float32>: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)}\n   1901         return updated[:len(self.outputs)]\n   1902 \n   1903 \n   1904 def function(inputs, outputs, updates=[], **kwargs):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in run(self=<tensorflow.python.client.session.Session object>, fetches=[<tf.Tensor 'mul_1:0' shape=() dtype=float32>, <tf.Tensor 'Mean_4:0' shape=() dtype=float32>, <tensorflow.python.framework.ops.Operation object>], feed_dict={<tf.Tensor 'dense_input_1:0' shape=(?, 8) dtype=float32>: array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), <tf.Tensor 'dense_2_target:0' shape=(?, ?) dtype=float32>: array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), <tf.Tensor 'dense_2_sample_weights:0' shape=(?,) dtype=float32>: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)}, options=None, run_metadata=None)\n    761     else:\n    762       options_ptr = None\n    763 \n    764     try:\n    765       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 766                          run_metadata_ptr)\n        run_metadata_ptr = <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >\n    767       if run_metadata:\n    768         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n    769         run_metadata.ParseFromString(compat.as_bytes(proto_data))\n    770     finally:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in _run(self=<tensorflow.python.client.session.Session object>, handle=None, fetches=[<tf.Tensor 'mul_1:0' shape=() dtype=float32>, <tf.Tensor 'Mean_4:0' shape=() dtype=float32>, <tensorflow.python.framework.ops.Operation object>], feed_dict={<tf.Tensor 'dense_input_1:0' shape=(?, 8) dtype=float32>: array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), <tf.Tensor 'dense_2_target:0' shape=(?, ?) dtype=float32>: array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), <tf.Tensor 'dense_2_sample_weights:0' shape=(?,) dtype=float32>: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)}, options=None, run_metadata=<tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >)\n    959     movers = self._update_with_movers(feed_dict_string, feed_map)\n    960     final_fetches = fetch_handler.fetches()\n    961     final_targets = fetch_handler.targets()\n    962     if final_fetches or final_targets:\n    963       results = self._do_run(handle, final_targets, final_fetches,\n--> 964                              feed_dict_string, options, run_metadata)\n        feed_dict_string = {'dense_2_sample_weights:0': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 'dense_2_target:0': array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ... 0.],\n       [ 1.],\n       [ 0.]], dtype=float32), 'dense_input_1:0': array([[  1.10000000e+01,   1.03000000e+02,   6.....50999993e-01,   5.50000000e+01]], dtype=float32)}\n        options = None\n        run_metadata = <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >\n    965     else:\n    966       results = []\n    967     return fetch_handler.build_results(self, results)\n    968 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in _do_run(self=<tensorflow.python.client.session.Session object>, handle=None, target_list=['group_deps'], fetch_list=['mul_1:0', 'Mean_4:0'], feed_dict={'dense_2_sample_weights:0': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 'dense_2_target:0': array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ... 0.],\n       [ 1.],\n       [ 0.]], dtype=float32), 'dense_input_1:0': array([[  1.10000000e+01,   1.03000000e+02,   6.....50999993e-01,   5.50000000e+01]], dtype=float32)}, options=None, run_metadata=<tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >)\n   1009         return tf_session.TF_PRun(session, handle, feed_dict, fetch_list,\n   1010                                   status)\n   1011 \n   1012     if handle is None:\n   1013       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n-> 1014                            target_list, options, run_metadata)\n        target_list = ['group_deps']\n        options = None\n        run_metadata = <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >\n   1015     else:\n   1016       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n   1017                            fetch_list)\n   1018 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in _do_call(self=<tensorflow.python.client.session.Session object>, fn=<function _run_fn>, *args=(<Swig Object of type 'TF_DeprecatedSession *'>, {'dense_2_sample_weights:0': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 'dense_2_target:0': array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ... 0.],\n       [ 1.],\n       [ 0.]], dtype=float32), 'dense_input_1:0': array([[  1.10000000e+01,   1.03000000e+02,   6.....50999993e-01,   5.50000000e+01]], dtype=float32)}, ['mul_1:0', 'Mean_4:0'], ['group_deps'], None, <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >))\n   1029         try:\n   1030           op = self._graph.get_operation_by_name(node_name)\n   1031           node_def = op.node_def\n   1032         except KeyError:\n   1033           pass\n-> 1034       raise type(e)(node_def, op, message)\n        e = InternalError()\n        node_def = name: \"MatMul\"\nop: \"MatMul\"\ninput: \"dense_input_... key: \"transpose_b\"\n  value {\n    b: false\n  }\n}\n\n        op = <tensorflow.python.framework.ops.Operation object>\n        message = u'Blas SGEMM launch failed : a.shape=(10, 8), b....:0\"](_recv_dense_input_1_0/_9, dense_1_W/read)]]'\n   1035 \n   1036   def _extend_graph(self):\n   1037     # Ensure any changes to the graph are reflected in the runtime.\n   1038     with self._extend_lock:\n\nInternalError: Blas SGEMM launch failed : a.shape=(10, 8), b.shape=(8, 12), m=10, n=12, k=8\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_input_1_0/_9, dense_1_W/read)]]\n\nCaused by op u'MatMul', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-a401dd13464c>\", line 31, in <module>\n    grid_result = grid.fit(X, Y)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py\", line 945, in fit\n    return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py\", line 564, in _fit\n    for parameters in parameter_iterable\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 728, in __call__\n    n_jobs = self._initialize_backend()\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 540, in _initialize_backend\n    **self._backend_args)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 311, in configure\n    self._pool = MemmapingPool(n_jobs, **backend_args)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/pool.py\", line 600, in __init__\n    super(MemmapingPool, self).__init__(**poolargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/pool.py\", line 420, in __init__\n    super(PicklingPool, self).__init__(**poolargs)\n  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 159, in __init__\n    self._repopulate_pool()\n  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 223, in _repopulate_pool\n    w.start()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 130, in start\n    self._popen = Popen(self)\n  File \"/usr/lib/python2.7/multiprocessing/forking.py\", line 126, in __init__\n    code = process_obj._bootstrap()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py\", line 238, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py\", line 134, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"<ipython-input-7-a401dd13464c>\", line 10, in create_model\n    model.add(Dense(12, input_dim=8, activation='relu'))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 299, in add\n    layer.create_input_layer(batch_input_shape, input_dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 401, in create_input_layer\n    self(x)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 572, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 635, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 166, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/core.py\", line 813, in call\n    output = K.dot(x, self.W)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 814, in dot\n    out = tf.matmul(x, y)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1729, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1442, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(10, 8), b.shape=(8, 12), m=10, n=12, k=8\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_input_1_0/_9, dense_1_W/read)]]\n\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJoblibInternalError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a401dd13464c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibInternalError\u001b[0m: JoblibInternalError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    169     pkg_name = mod_name.rpartition('.')[0]\n    170     main_globals = sys.modules[\"__main__\"].__dict__\n    171     if alter_argv:\n    172         sys.argv[0] = fname\n    173     return _run_code(code, main_globals, None,\n--> 174                      \"__main__\", fname, loader, pkg_name)\n        fname = '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    175 \n    176 def run_module(mod_name, init_globals=None,\n    177                run_name=None, alter_sys=False):\n    178     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7f3ea27c3430, file \"/...2.7/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    648 \n    649         If a global instance already exists, this reinitializes and starts it\n    650         \"\"\"\n    651         app = cls.instance(**kwargs)\n    652         app.initialize(argv)\n--> 653         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    654 \n    655 #-----------------------------------------------------------------------------\n    656 # utility functions, for convenience\n    657 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:23:00.203236', u'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', 'msg_type': u'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['B64E8C726E0848628D7C1B41F48E4D5D']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:23:00.203236', u'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', 'msg_type': u'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['B64E8C726E0848628D7C1B41F48E4D5D'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2017-02-06T15:23:00.203236', u'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', u'msg_type': u'execute_request', u'session': u'B64E8C726E0848628D7C1B41F48E4D5D', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'14B744011F804B869ACDE0DF7F05B29F', 'msg_type': u'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))',), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))',)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Import object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.FunctionDef object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Print object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, ...], cell_name='<ipython-input-7-a401dd13464c>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<ExecutionResult object at 7f3e7ff2d310, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f3e7ff637b0, file \"<ipython-input-7-a401dd13464c>\", line 31>\n        result = <ExecutionResult object at 7f3e7ff2d310, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f3e7ff637b0, file \"<ipython-input-7-a401dd13464c>\", line 31>, result=<ExecutionResult object at 7f3e7ff2d310, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f3e7ff637b0, file \"<ipython-input-7-a401dd13464c>\", line 31>\n        self.user_global_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n        self.user_ns = {'BaseWrapper': <class 'keras.wrappers.scikit_learn.BaseWrapper'>, 'Dense': <class 'keras.layers.core.Dense'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', u'import numpy\\nfrom sklearn.model_selection imp...on to create model, required for KerasClassifier', u\"from keras.wrappers.scikit_learn import BaseWr...es\\n\\nBaseWrapper.get_params = custom_get_params\", u'\\ndef create_model():\\n    # create model\\n   ...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))', u'import numpy\\nfrom sklearn.model_selection imp...print(\"%f (%f) with: %r\" % (mean, stdev, param))'], 'KerasClassifier': <class 'keras.wrappers.scikit_learn.KerasClassifier'>, 'Out': {}, 'Sequential': <class 'keras.models.Sequential'>, 'X': array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), 'Y': array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), '_': '', ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/garth/numerai_notebooks/<ipython-input-7-a401dd13464c> in <module>()\n     26 model = KerasClassifier(build_fn=create_model, nb_epoch=100, batch_size=10, verbose=0)\n     27 # define the grid search parameters\n     28 optimizer = ['SGD', 'RMSprop', 'Adagrad']\n     29 param_grid = dict(optimizer=optimizer)\n     30 grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n---> 31 grid_result = grid.fit(X, Y)\n     32 # summarize results\n     33 print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n     34 means = grid_result.cv_results_['mean_test_score']\n     35 stds = grid_result.cv_results_['std_test_score']\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=0)>\n        X = array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]])\n        y = array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.])\n        groups = None\n        self.param_grid = {'optimizer': ['SGD', 'RMSprop', 'Adagrad']}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...train_score=True,\n       scoring=None, verbose=0), X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nInternalError                                      Mon Feb  6 15:23:02 2017\nPID: 8503                                    Python 2.7.12: /usr/bin/python\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'optimizer': 'SGD'})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n        self.items = [(<function _fit_and_score>, (<keras.wrappers.scikit_learn.KerasClassifier object>, array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), <function _passthrough_scorer>, array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), 0, {'optimizer': 'SGD'}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=<keras.wrappers.scikit_learn.KerasClassifier object>, X=array([[   6.   ,  148.   ,   72.   , ...,   33.... ,   70.   , ...,   30.4  ,    0.315,   23.   ]]), y=array([ 1.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  ...0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.]), scorer=<function _passthrough_scorer>, train=array([256, 257, 258, 259, 260, 261, 262, 263, 2..., 760, 761, 762,\n       763, 764, 765, 766, 767]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    247, 248, 249, 250, 251, 252, 253, 254, 255]), verbose=0, parameters={'optimizer': 'SGD'}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    233 \n    234     try:\n    235         if y_train is None:\n    236             estimator.fit(X_train, **fit_params)\n    237         else:\n--> 238             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method KerasClassifier.fit of <keras.wrappers.scikit_learn.KerasClassifier object>>\n        X_train = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y_train = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_params = {}\n    239 \n    240     except Exception as e:\n    241         # Note fit time as time until error\n    242         fit_time = time.time() - start_time\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py in fit(self=<keras.wrappers.scikit_learn.KerasClassifier object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), **kwargs={})\n    140             y = to_categorical(y)\n    141 \n    142         fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n    143         fit_args.update(kwargs)\n    144 \n--> 145         history = self.model.fit(x, y, **fit_args)\n        history = undefined\n        self.model.fit = <bound method Sequential.fit of <keras.models.Sequential object>>\n        x = array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])\n        y = array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.])\n        fit_args = {'batch_size': 10, 'nb_epoch': 100, 'verbose': 0}\n    146 \n    147         return history\n    148 \n    149     def filter_sk_params(self, fn, override=None):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/models.py in fit(self=<keras.models.Sequential object>, x=array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), y=array([ 0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  ...,  0.,  1.,  0.,\n        0.,  0.,  0.,  1.,  0.]), batch_size=10, nb_epoch=100, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, **kwargs={})\n    667                               validation_split=validation_split,\n    668                               validation_data=validation_data,\n    669                               shuffle=shuffle,\n    670                               class_weight=class_weight,\n    671                               sample_weight=sample_weight,\n--> 672                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n    673 \n    674     def evaluate(self, x, y, batch_size=32, verbose=1,\n    675                  sample_weight=None, **kwargs):\n    676         \"\"\"Computes the loss on some input data, batch by batch.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in fit(self=<keras.engine.training.Model object>, x=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]])], y=[array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]])], batch_size=10, nb_epoch=100, verbose=0, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n   1187         return self._fit_loop(f, ins, out_labels=out_labels,\n   1188                               batch_size=batch_size, nb_epoch=nb_epoch,\n   1189                               verbose=verbose, callbacks=callbacks,\n   1190                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n   1191                               callback_metrics=callback_metrics,\n-> 1192                               initial_epoch=initial_epoch)\n        initial_epoch = 0\n   1193 \n   1194     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n   1195         \"\"\"Returns the loss value and metrics values for the model\n   1196         in test mode. Computation is done in batches.\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.py in _fit_loop(self=<keras.engine.training.Model object>, f=<keras.backend.tensorflow_backend.Function object>, ins=[array([[  3.00000000e+00,   1.11000000e+02,   5....000000e+01,   3.15000000e-01,   2.30000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 0.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  ...,\n        1.,  1.,  1.,  1.,  1.], dtype=float32)], out_labels=['loss', 'acc'], batch_size=10, nb_epoch=100, verbose=0, callbacks=<keras.callbacks.CallbackList object>, val_f=None, val_ins=None, shuffle=True, callback_metrics=['loss', 'acc'], initial_epoch=0)\n    887                                     'pass shuffle=\"batch\".')\n    888                 batch_logs = {}\n    889                 batch_logs['batch'] = batch_index\n    890                 batch_logs['size'] = len(batch_ids)\n    891                 callbacks.on_batch_begin(batch_index, batch_logs)\n--> 892                 outs = f(ins_batch)\n        outs = undefined\n        f = <keras.backend.tensorflow_backend.Function object>\n        ins_batch = [array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)]\n    893                 if not isinstance(outs, list):\n    894                     outs = [outs]\n    895                 for l, o in zip(out_labels, outs):\n    896                     batch_logs[l] = o\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py in __call__(self=<keras.backend.tensorflow_backend.Function object>, inputs=[array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)])\n   1895                                           np.expand_dims(sparse_coo.col, 1)), 1)\n   1896                 value = (indices, sparse_coo.data, sparse_coo.shape)\n   1897             feed_dict[tensor] = value\n   1898         session = get_session()\n   1899         updated = session.run(self.outputs + [self.updates_op],\n-> 1900                               feed_dict=feed_dict)\n        feed_dict = {<tf.Tensor 'dense_input_1:0' shape=(?, 8) dtype=float32>: array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), <tf.Tensor 'dense_2_target:0' shape=(?, ?) dtype=float32>: array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), <tf.Tensor 'dense_2_sample_weights:0' shape=(?,) dtype=float32>: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)}\n   1901         return updated[:len(self.outputs)]\n   1902 \n   1903 \n   1904 def function(inputs, outputs, updates=[], **kwargs):\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in run(self=<tensorflow.python.client.session.Session object>, fetches=[<tf.Tensor 'mul_1:0' shape=() dtype=float32>, <tf.Tensor 'Mean_4:0' shape=() dtype=float32>, <tensorflow.python.framework.ops.Operation object>], feed_dict={<tf.Tensor 'dense_input_1:0' shape=(?, 8) dtype=float32>: array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), <tf.Tensor 'dense_2_target:0' shape=(?, ?) dtype=float32>: array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), <tf.Tensor 'dense_2_sample_weights:0' shape=(?,) dtype=float32>: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)}, options=None, run_metadata=None)\n    761     else:\n    762       options_ptr = None\n    763 \n    764     try:\n    765       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 766                          run_metadata_ptr)\n        run_metadata_ptr = <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >\n    767       if run_metadata:\n    768         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n    769         run_metadata.ParseFromString(compat.as_bytes(proto_data))\n    770     finally:\n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in _run(self=<tensorflow.python.client.session.Session object>, handle=None, fetches=[<tf.Tensor 'mul_1:0' shape=() dtype=float32>, <tf.Tensor 'Mean_4:0' shape=() dtype=float32>, <tensorflow.python.framework.ops.Operation object>], feed_dict={<tf.Tensor 'dense_input_1:0' shape=(?, 8) dtype=float32>: array([[  1.10000000e+01,   1.03000000e+02,   6....01,\n          1.51000000e-01,   5.50000000e+01]]), <tf.Tensor 'dense_2_target:0' shape=(?, ?) dtype=float32>: array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ...[ 1.],\n       [ 0.],\n       [ 1.],\n       [ 0.]]), <tf.Tensor 'dense_2_sample_weights:0' shape=(?,) dtype=float32>: array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)}, options=None, run_metadata=<tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >)\n    959     movers = self._update_with_movers(feed_dict_string, feed_map)\n    960     final_fetches = fetch_handler.fetches()\n    961     final_targets = fetch_handler.targets()\n    962     if final_fetches or final_targets:\n    963       results = self._do_run(handle, final_targets, final_fetches,\n--> 964                              feed_dict_string, options, run_metadata)\n        feed_dict_string = {'dense_2_sample_weights:0': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 'dense_2_target:0': array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ... 0.],\n       [ 1.],\n       [ 0.]], dtype=float32), 'dense_input_1:0': array([[  1.10000000e+01,   1.03000000e+02,   6.....50999993e-01,   5.50000000e+01]], dtype=float32)}\n        options = None\n        run_metadata = <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >\n    965     else:\n    966       results = []\n    967     return fetch_handler.build_results(self, results)\n    968 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in _do_run(self=<tensorflow.python.client.session.Session object>, handle=None, target_list=['group_deps'], fetch_list=['mul_1:0', 'Mean_4:0'], feed_dict={'dense_2_sample_weights:0': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 'dense_2_target:0': array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ... 0.],\n       [ 1.],\n       [ 0.]], dtype=float32), 'dense_input_1:0': array([[  1.10000000e+01,   1.03000000e+02,   6.....50999993e-01,   5.50000000e+01]], dtype=float32)}, options=None, run_metadata=<tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >)\n   1009         return tf_session.TF_PRun(session, handle, feed_dict, fetch_list,\n   1010                                   status)\n   1011 \n   1012     if handle is None:\n   1013       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n-> 1014                            target_list, options, run_metadata)\n        target_list = ['group_deps']\n        options = None\n        run_metadata = <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >\n   1015     else:\n   1016       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n   1017                            fetch_list)\n   1018 \n\n...........................................................................\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py in _do_call(self=<tensorflow.python.client.session.Session object>, fn=<function _run_fn>, *args=(<Swig Object of type 'TF_DeprecatedSession *'>, {'dense_2_sample_weights:0': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 'dense_2_target:0': array([[ 0.],\n       [ 0.],\n       [ 0.],\n      ... 0.],\n       [ 1.],\n       [ 0.]], dtype=float32), 'dense_input_1:0': array([[  1.10000000e+01,   1.03000000e+02,   6.....50999993e-01,   5.50000000e+01]], dtype=float32)}, ['mul_1:0', 'Mean_4:0'], ['group_deps'], None, <tensorflow.python.pywrap_tensorflow.TF_Buffer; ...Object of type 'TF_Buffer *' at 0x7f3e7ff5e420> >))\n   1029         try:\n   1030           op = self._graph.get_operation_by_name(node_name)\n   1031           node_def = op.node_def\n   1032         except KeyError:\n   1033           pass\n-> 1034       raise type(e)(node_def, op, message)\n        e = InternalError()\n        node_def = name: \"MatMul\"\nop: \"MatMul\"\ninput: \"dense_input_... key: \"transpose_b\"\n  value {\n    b: false\n  }\n}\n\n        op = <tensorflow.python.framework.ops.Operation object>\n        message = u'Blas SGEMM launch failed : a.shape=(10, 8), b....:0\"](_recv_dense_input_1_0/_9, dense_1_W/read)]]'\n   1035 \n   1036   def _extend_graph(self):\n   1037     # Ensure any changes to the graph are reflected in the runtime.\n   1038     with self._extend_lock:\n\nInternalError: Blas SGEMM launch failed : a.shape=(10, 8), b.shape=(8, 12), m=10, n=12, k=8\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_input_1_0/_9, dense_1_W/read)]]\n\nCaused by op u'MatMul', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-a401dd13464c>\", line 31, in <module>\n    grid_result = grid.fit(X, Y)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py\", line 945, in fit\n    return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py\", line 564, in _fit\n    for parameters in parameter_iterable\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 728, in __call__\n    n_jobs = self._initialize_backend()\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 540, in _initialize_backend\n    **self._backend_args)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 311, in configure\n    self._pool = MemmapingPool(n_jobs, **backend_args)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/pool.py\", line 600, in __init__\n    super(MemmapingPool, self).__init__(**poolargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/pool.py\", line 420, in __init__\n    super(PicklingPool, self).__init__(**poolargs)\n  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 159, in __init__\n    self._repopulate_pool()\n  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 223, in _repopulate_pool\n    w.start()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 130, in start\n    self._popen = Popen(self)\n  File \"/usr/lib/python2.7/multiprocessing/forking.py\", line 126, in __init__\n    code = process_obj._bootstrap()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 113, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py\", line 238, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py\", line 134, in fit\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n  File \"<ipython-input-7-a401dd13464c>\", line 10, in create_model\n    model.add(Dense(12, input_dim=8, activation='relu'))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 299, in add\n    layer.create_input_layer(batch_input_shape, input_dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 401, in create_input_layer\n    self(x)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 572, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 635, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 166, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/core.py\", line 813, in call\n    output = K.dot(x, self.W)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 814, in dot\n    out = tf.matmul(x, y)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1729, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1442, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(10, 8), b.shape=(8, 12), m=10, n=12, k=8\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dense_input_1_0/_9, dense_1_W/read)]]\n\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"/home/garth/Downloads/pima-indians-diabetes.data.txt\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, nb_epoch=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
